{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HealthHabit - Hospital Multiclassification Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Recent COVID-19 pandemic has raised alarms over one of the most overlooked areas to focus on: Healthcare Management. While healthcare management has various use cases for using data science, patient length of stay or LOS in short is one critical parameter to observe and predict if one wants to improve the efficiency of healthcare management in a hospital. \n",
    "\n",
    "\n",
    "This parameter helps hospitals to identify patients of high LOS risk (patients who will stay longer) at the time of admission. Once identified, patients with high LOS risk can have their treatment plan optimized to minimize LOS and lower the chance of staff/visitor infection. Also, prior knowledge of LOS can aid in logistics such as room and bed allocation planning.Ypose you have been hired as a Data Scieforist at HealthHabitat â€“ a not-for-profit organization dedicated to managing the functioning of Hospitals professionally and optimally.\n",
    "The task is to accurately predict the Length of Stay for each patient on a case-by-case basis so that the Hospitals can use this information for optimal resource allocation and better function\n",
    "PREDICT: The length of stay is divided into 11 different classes ranging from 0-10 days to more than 100 days.   \n",
    " days.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's go ahead and import our dataset first and take a look at the columns present in it. Note that i will not be running the codes in real time, owing to the processing time and the size of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('hospital_stay_data.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for null values in our dataset and verify the type of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 318438 entries, 0 to 318437\n",
      "Data columns (total 18 columns):\n",
      " #   Column                             Non-Null Count   Dtype  \n",
      "---  ------                             --------------   -----  \n",
      " 0   case_id                            318438 non-null  int64  \n",
      " 1   Hospital_code                      318438 non-null  int64  \n",
      " 2   Hospital_type_code                 318438 non-null  object \n",
      " 3   City_Code_Hospital                 318438 non-null  int64  \n",
      " 4   Hospital_region_code               318438 non-null  object \n",
      " 5   Available Extra Rooms in Hospital  318438 non-null  int64  \n",
      " 6   Department                         318438 non-null  object \n",
      " 7   Ward_Type                          318438 non-null  object \n",
      " 8   Ward_Facility_Code                 318438 non-null  object \n",
      " 9   Bed Grade                          318325 non-null  float64\n",
      " 10  patientid                          318438 non-null  int64  \n",
      " 11  City_Code_Patient                  313906 non-null  float64\n",
      " 12  Type of Admission                  318438 non-null  object \n",
      " 13  Severity of Illness                318438 non-null  object \n",
      " 14  Visitors with Patient              318438 non-null  int64  \n",
      " 15  Age                                318438 non-null  object \n",
      " 16  Admission_Deposit                  318438 non-null  float64\n",
      " 17  Stay                               318438 non-null  object \n",
      "dtypes: float64(3), int64(6), object(9)\n",
      "memory usage: 43.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly move onto data preprocessing. Just like we did earlier, first lets look at the columns with null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with NaNs: ['Bed Grade', 'City_Code_Patient']\n"
     ]
    }
   ],
   "source": [
    "# Find columns with NaN values\n",
    "columns_with_nans = df.columns[df.isna().any()].tolist()\n",
    "\n",
    "# Print the names of columns with NaNs\n",
    "print(\"Columns with NaNs:\", columns_with_nans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop rows with missing values from these columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are very few missing values compared to the size of the data. Here, only the two columns have null values, let's drop the rows from the bed grade and city code patient columns from the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "case_id                              313793\n",
       "Hospital_code                        313793\n",
       "Hospital_type_code                   313793\n",
       "City_Code_Hospital                   313793\n",
       "Hospital_region_code                 313793\n",
       "Available Extra Rooms in Hospital    313793\n",
       "Department                           313793\n",
       "Ward_Type                            313793\n",
       "Ward_Facility_Code                   313793\n",
       "Bed Grade                            313793\n",
       "patientid                            313793\n",
       "City_Code_Patient                    313793\n",
       "Type of Admission                    313793\n",
       "Severity of Illness                  313793\n",
       "Visitors with Patient                313793\n",
       "Age                                  313793\n",
       "Admission_Deposit                    313793\n",
       "Stay                                 313793\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's check the data distribution for each of the 11 classes to decide on the most suitable evaluation metric for this problem statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution (%):\n",
      " Stay\n",
      "21-30                 27.507306\n",
      "11-20                 24.568744\n",
      "31-40                 17.308225\n",
      "51-60                 10.982718\n",
      "0-10                   7.409343\n",
      "41-50                  3.677902\n",
      "71-80                  3.217408\n",
      "More than 100 Days     2.086726\n",
      "81-90                  1.517242\n",
      "91-100                 0.864583\n",
      "61-70                  0.859802\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# The column \"Stay\" is what we have to predict. Check the distribution of the 'Stay' column\n",
    "class_distribution = df['Stay'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"Class Distribution (%):\\n\", class_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data is distributed among the 11 different classes, we can go ahead with accuracy as our evaluation metric for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's identify the categorical and numerical columns and proceed to transform them. Here we're going to use a standard scaler for the numerical columns and one hot encoder for the catergorical columns since we've a large number of rows in this dataset. After that we shall split our data into train and test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify the categorical and numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Encode categorical variables and normalize numerical variables\n",
    "categorical_columns = ['Hospital_code', 'Hospital_type_code', 'City_Code_Hospital', 'Hospital_region_code', 'Department', 'Ward_Type', 'Ward_Facility_Code', 'Bed Grade', 'City_Code_Patient','Type of Admission', 'Severity of Illness', 'Age']\n",
    "numerical_columns = ['Available Extra Rooms in Hospital', 'Visitors with Patient', 'Admission_Deposit']\n",
    "\n",
    "# Encoding and Normalizing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_columns),\n",
    "        ('cat', OneHotEncoder(), categorical_columns)\n",
    "    ])\n",
    "\n",
    "# Preparing target\n",
    "label_encoder = LabelEncoder()\n",
    "df['Stay'] = label_encoder.fit_transform(df['Stay'])\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Splitting the dataset\n",
    "X = df.drop(['patientid','Stay'], axis=1)\n",
    "y = df['Stay']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply preprocessing: create a model based on the dataset and then transform the dataset based on the created model\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that we've completed the data preprocessing. Next, let's convert them into tensors using Tensor dataset and dataloaders. Also lets use a batch_size of 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train.toarray().astype(np.float32))\n",
    "y_train_tensor = torch.tensor(y_train.values.astype(np.int64))\n",
    "X_test_tensor = torch.tensor(X_test.toarray().astype(np.float32))\n",
    "y_test_tensor = torch.tensor(y_test.values.astype(np.int64))\n",
    "\n",
    "# Prepare DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define our neural network. Here I've defined my neural network with ReLu activation function in both layers. Notice that, there is no activation function in the last layer. We would be using a softmax activation function on the output of the last layer, since this is a multiclassification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our neural network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HospitalStayNet(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(HospitalStayNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_features, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate our neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set basic parameters like number of features and classes, and instantiate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Determine the number of features from the column count of the dataset\n",
    "num_features = X_train.shape[1]\n",
    "num_classes = 11  # As mentioned, there are 11 classes\n",
    "# Use GPU, if it is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "model = HospitalStayNet(num_features, num_classes).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define the function that calculates our trained model's accuracy. In this code, the function sets the model to evaluation mode, iterates over batches of input data, computes the model's outputs, identifies the class with the highest prediction score, and compares it with the true labels to tally the correct predictions, returning the percentage accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the function that calculates our trained model's accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "def calculate_accuracy(loader):\n",
    "    model.eval()  # Set the model to evaluation mode. \n",
    "\n",
    "    correct = 0  # the number of correct predictions.\n",
    "    total = 0  # the total number of predictions.\n",
    "\n",
    "    with torch.no_grad():  # Disable the gradient calculation to save memory and speed up the process since gradients are not needed for evaluation.\n",
    "        # Iterate over the data loader, which provides batches of inputs and their corresponding targets.\n",
    "        for inputs, targets in loader:  \n",
    "            inputs, targets = inputs.to(device), targets.to(device)  \n",
    "\n",
    "            outputs = model(inputs)  # Compute the model's outputs for the given inputs.\n",
    "\n",
    "            # Find the predicted class with the highest score for each input. \n",
    "            # The `torch.max` function returns both the maximum values and their indices (the predicted classes)\n",
    "            _, predicted = torch.max(outputs.data, 1)  \n",
    "            # targets.size(0) gives the number of targets in the batch.\n",
    "            total += targets.size(0)  \n",
    "            # Calculate the number of correct predictions in the batch by comparing predicted with targets, summing the true predictions, and adding this sum to the correct counter.\n",
    "            correct += (predicted == targets).sum().item()  \n",
    "\n",
    "    return 100 * correct / total  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the loss function and the optimizer to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the Softmax activation for our multiclass classification problem is automatically applied in pytorch's CrossEntropyLoss function during computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've setup our model successfully. So let's go ahead and train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training our model, i've set the epochs to 20 to once again reduce the processing time. Further i've also added early stopping to this model with the max_patience set to 3. Note that, over here, i've also added an additional line of code to save the model with the best accuracy (HL). We will be using different names for the best model for the different architectures that we try. Take a look at the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.6718, Train Accuracy: 44.10%, Test Accuracy: 42.81%\n",
      "Epoch 2, Loss: 1.4220, Train Accuracy: 44.05%, Test Accuracy: 42.61%\n",
      "Epoch 3, Loss: 1.4790, Train Accuracy: 44.09%, Test Accuracy: 42.81%\n",
      "Epoch 4, Loss: 1.3952, Train Accuracy: 44.27%, Test Accuracy: 42.78%\n",
      "Epoch 5, Loss: 1.3607, Train Accuracy: 44.29%, Test Accuracy: 42.56%\n",
      "Early stopped at 6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "best_accuracy = float(0.01)\n",
    "patience = 0\n",
    "max_patience = 3  # Maximum epochs to wait for improvement\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()       # Set the model to training mode. \n",
    "    # Iterate over the data loader, which provides batches of inputs and their corresponding targets.\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad() # initialize the gradients for this batch of data\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets) # calculate the losses\n",
    "        loss.backward()  # compute the gradients based on the loss values\n",
    "        optimizer.step() # update the weights and biases based on the loss values\n",
    "    \n",
    "    train_accuracy = calculate_accuracy(train_loader)\n",
    "    test_accuracy = calculate_accuracy(test_loader)\n",
    "    if test_accuracy > best_accuracy:\n",
    "        best_accuracy = test_accuracy\n",
    "        patience = 0\n",
    "        torch.save(model, 'best_model_simple.pt')\n",
    "    else:\n",
    "        patience += 1\n",
    "\n",
    "    if patience >= max_patience:\n",
    "        print(f'Early stopped at {epoch+1}')\n",
    "        break  # Stop training\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}, Train Accuracy: {train_accuracy:.2f}%, Test Accuracy: {test_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can observe that even though we had set the num of epochs to 20, we have got the best performance of 42.81% on the test set in the 3rd epoch. After the 3rd epoch, since the performance did not improve in the next 3 epochs, the model stopped after the 6th epoch. Feel free to increase the number of epochs and the max_patience to see if you get better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's try and improve our models performance by increasing the number of layers and nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a newer model with more layers and nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HospitalStayNet256(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(HospitalStayNet256, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_features, 128)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.fc5 = nn.Linear(64, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "model = HospitalStayNet256(num_features, num_classes).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the new model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again for this larger model i've gone with the same setup as earlier, that is, 20 epochs, a max patience of 3 and to save the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.3710, Train Accuracy: 42.51%, Test Accuracy: 42.80%\n",
      "Epoch 2, Loss: 1.4949, Train Accuracy: 42.64%, Test Accuracy: 42.78%\n",
      "Epoch 3, Loss: 1.3632, Train Accuracy: 42.66%, Test Accuracy: 42.62%\n",
      "Epoch 4, Loss: 1.5679, Train Accuracy: 42.94%, Test Accuracy: 42.89%\n",
      "Epoch 5, Loss: 1.1875, Train Accuracy: 43.36%, Test Accuracy: 42.91%\n",
      "Epoch 6, Loss: 1.2564, Train Accuracy: 43.50%, Test Accuracy: 43.00%\n",
      "Epoch 7, Loss: 1.2125, Train Accuracy: 43.88%, Test Accuracy: 43.10%\n",
      "Epoch 8, Loss: 1.3437, Train Accuracy: 44.07%, Test Accuracy: 43.11%\n",
      "Epoch 9, Loss: 1.6099, Train Accuracy: 44.10%, Test Accuracy: 42.76%\n",
      "Epoch 10, Loss: 1.6654, Train Accuracy: 44.42%, Test Accuracy: 42.86%\n",
      "Early stopped at 11\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "best_accuracy = float(0.01)\n",
    "patience = 0\n",
    "max_patience = 3  # Maximum epochs to wait for improvement\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()       # Set the model to training mode. \n",
    "    # Iterate over the data loader, which provides batches of inputs and their corresponding targets.\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad() # initialize the gradients for this batch of data\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets) # calculate the losses\n",
    "        loss.backward()  # compute the gradients based on the loss values\n",
    "        optimizer.step() # update the weights and biases based on the loss values\n",
    "    \n",
    "    train_accuracy = calculate_accuracy(train_loader)\n",
    "    test_accuracy = calculate_accuracy(test_loader)\n",
    "    if test_accuracy > best_accuracy:\n",
    "        best_accuracy = test_accuracy\n",
    "        patience = 0\n",
    "        torch.save(model, 'best_model_big.pt')\n",
    "    else:\n",
    "        patience += 1\n",
    "\n",
    "    if patience >= max_patience:\n",
    "        print(f'Early stopped at {epoch+1}')\n",
    "        break  # Stop training\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}, Train Accuracy: {train_accuracy:.2f}%, Test Accuracy: {test_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the more complex model achieves a slightly better test accuracy. The accuracy has improved from 42.81 to 43.11 in the 8th epoch. Although if you look at the 10th epoch, it appears the model is slightly overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's try different techniques such as dropout and batch normalization to see if the performance can be improved further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the model performance with dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define another model with dropout layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HospitalStayNetWithDropout(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(HospitalStayNetWithDropout, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_features, 128)\n",
    "        self.dropout1 = nn.Dropout(0.5)  # Dropout layer with 50% probability\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        #self.dropout2 = nn.Dropout(0.5)  # Another Dropout layer\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = HospitalStayNetWithDropout(num_features, num_classes).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model with the dropout layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.0530, Train Accuracy: 41.28%, Test Accuracy: 41.62%\n",
      "Epoch 2, Loss: 1.7034, Train Accuracy: 42.16%, Test Accuracy: 42.47%\n",
      "Epoch 3, Loss: 1.5578, Train Accuracy: 42.42%, Test Accuracy: 42.58%\n",
      "Epoch 4, Loss: 1.5634, Train Accuracy: 42.50%, Test Accuracy: 42.64%\n",
      "Epoch 5, Loss: 1.7391, Train Accuracy: 42.63%, Test Accuracy: 42.83%\n",
      "Epoch 6, Loss: 1.7118, Train Accuracy: 42.61%, Test Accuracy: 42.73%\n",
      "Epoch 7, Loss: 1.5481, Train Accuracy: 42.88%, Test Accuracy: 43.04%\n",
      "Epoch 8, Loss: 1.3582, Train Accuracy: 42.76%, Test Accuracy: 42.95%\n",
      "Epoch 9, Loss: 1.5448, Train Accuracy: 42.81%, Test Accuracy: 42.88%\n",
      "Early stopped at 10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "best_accuracy = float(0.01)\n",
    "patience = 0\n",
    "max_patience = 3  # Maximum epochs to wait for improvement\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()       # Set the model to training mode. \n",
    "    # Iterate over the data loader, which provides batches of inputs and their corresponding targets.\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad() # initialize the gradients for this batch of data\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets) # calculate the losses\n",
    "        loss.backward()  # compute the gradients based on the loss values\n",
    "        optimizer.step() # update the weights and biases based on the loss values\n",
    "    \n",
    "    train_accuracy = calculate_accuracy(train_loader)\n",
    "    test_accuracy = calculate_accuracy(test_loader)\n",
    "    if test_accuracy > best_accuracy:\n",
    "        best_accuracy = test_accuracy\n",
    "        patience = 0\n",
    "        torch.save(model, 'best_model_dropout.pt')\n",
    "    else:\n",
    "        patience += 1\n",
    "\n",
    "    if patience >= max_patience:\n",
    "        print(f'Early stopped at {epoch+1}')\n",
    "        break  # Stop training\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}, Train Accuracy: {train_accuracy:.2f}%, Test Accuracy: {test_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we've got our best performance in the 7th epoch. Note that although the model with dropout layers has lesser test accuracy at 43.04% compared to the earlier model, the training and test scores and very close now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly try batch normalization too and analyze the models performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define another model with Batch Normalization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HospitalStayNetWithBN(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(HospitalStayNetWithBN, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_features, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = HospitalStayNetWithBN(num_features, num_classes).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model with batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.5455, Train Accuracy: 42.21%, Test Accuracy: 42.35%\n",
      "Epoch 2, Loss: 1.7564, Train Accuracy: 42.61%, Test Accuracy: 42.47%\n",
      "Epoch 3, Loss: 1.5042, Train Accuracy: 42.93%, Test Accuracy: 42.91%\n",
      "Epoch 4, Loss: 1.2331, Train Accuracy: 43.09%, Test Accuracy: 42.87%\n",
      "Epoch 5, Loss: 1.6523, Train Accuracy: 43.07%, Test Accuracy: 42.69%\n",
      "Early stopped at 6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "best_accuracy = float(0.01)\n",
    "patience = 0\n",
    "max_patience = 3  # Maximum epochs to wait for improvement\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()       # Set the model to training mode. \n",
    "    # Iterate over the data loader, which provides batches of inputs and their corresponding targets.\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad() # initialize the gradients for this batch of data\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets) # calculate the losses\n",
    "        loss.backward()  # compute the gradients based on the loss values\n",
    "        optimizer.step() # update the weights and biases based on the loss values\n",
    "    \n",
    "    train_accuracy = calculate_accuracy(train_loader)\n",
    "    test_accuracy = calculate_accuracy(test_loader)\n",
    "    if test_accuracy > best_accuracy:\n",
    "        best_accuracy = test_accuracy\n",
    "        patience = 0\n",
    "        torch.save(model, 'best_model_BN.pt')\n",
    "    else:\n",
    "        patience += 1\n",
    "\n",
    "    if patience >= max_patience:\n",
    "        print(f'Early stopped at {epoch+1}')\n",
    "        break  # Stop training\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}, Train Accuracy: {train_accuracy:.2f}%, Test Accuracy: {test_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with batch normalization has not superseded the performance of the dropout model we created earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's try hypertune the model parameters using the Optuna library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization with Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a framework model with 2 hidden layers. You can try tuning a larger network as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class HospitalStayNetOptuna(nn.Module):\n",
    "    def __init__(self, num_features, num_classes, fc1_units=128, fc2_units=64):\n",
    "        super(HospitalStayNetOptuna, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_features, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's slightly modify the calculate_accuracy function to take different models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy_optuna(loader, model):\n",
    "    model.eval()  # Set the model to evaluation mode. \n",
    "\n",
    "    correct = 0  # the number of correct predictions.\n",
    "    total = 0  # the total number of predictions.\n",
    "\n",
    "    with torch.no_grad():  # Disable the gradient calculation to save memory and speed up the process since gradients are not needed for evaluation.\n",
    "        # Iterate over the data loader, which provides batches of inputs and their corresponding targets.\n",
    "        for inputs, targets in loader:  \n",
    "            inputs, targets = inputs.to(device), targets.to(device)  \n",
    "\n",
    "            outputs = model(inputs)  # Compute the model's outputs for the given inputs.\n",
    "\n",
    "            # Find the predicted class with the highest score for each input. \n",
    "            # The `torch.max` function returns both the maximum values and their indices (the predicted classes)\n",
    "            _, predicted = torch.max(outputs.data, 1)  \n",
    "            # targets.size(0) gives the number of targets in the batch.\n",
    "            total += targets.size(0)  \n",
    "            # Calculate the number of correct predictions in the batch by comparing predicted with targets, summing the true predictions, and adding this sum to the correct counter.\n",
    "            correct += (predicted == targets).sum().item()  \n",
    "\n",
    "    return 100 * correct / total  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to tune the learning rate, number of hidden units in each of the two layers and the batch size. These parameters will be tuned to optimize for the objective of finding the model with the best test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Owing to the time taken in processing, I've choosen to go with only 10 epochs and 10 n_trails for hyperparameter tuning. Feel free to increase the number of epochs to a higher value. Note that this will significantly increase the processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apoorv/opt/anaconda3/envs/pytorch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2024-03-08 16:54:34,688] A new study created in memory with name: no-name-dffa987c-2545-4d4b-8cf4-b9c056e8f505\n",
      "[I 2024-03-08 16:55:24,196] Trial 0 finished with value: 42.11666852562979 and parameters: {'lr': 6.105955332454353e-05, 'optimizer': 'Adam', 'fc1_units': 64, 'fc2_units': 32, 'batch_size': 64}. Best is trial 0 with value: 42.11666852562979.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Test Accuracy: 42.12%\n",
      "Finished trial #0 with value: 42.12%\n",
      "Best parameters: {'lr': 6.105955332454353e-05, 'optimizer': 'Adam', 'fc1_units': 64, 'fc2_units': 32, 'batch_size': 64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-08 16:56:00,542] Trial 1 finished with value: 42.53891872082092 and parameters: {'lr': 0.010122234735374186, 'optimizer': 'Adam', 'fc1_units': 64, 'fc2_units': 32, 'batch_size': 128}. Best is trial 1 with value: 42.53891872082092.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Test Accuracy: 42.54%\n",
      "Finished trial #1 with value: 42.54%\n",
      "Best parameters: {'lr': 0.010122234735374186, 'optimizer': 'Adam', 'fc1_units': 64, 'fc2_units': 32, 'batch_size': 128}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-08 16:56:56,445] Trial 2 finished with value: 40.34799789671601 and parameters: {'lr': 1.1390336685532758e-05, 'optimizer': 'Adam', 'fc1_units': 64, 'fc2_units': 128, 'batch_size': 64}. Best is trial 1 with value: 42.53891872082092.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Test Accuracy: 40.35%\n",
      "Finished trial #1 with value: 42.54%\n",
      "Best parameters: {'lr': 0.010122234735374186, 'optimizer': 'Adam', 'fc1_units': 64, 'fc2_units': 32, 'batch_size': 128}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-08 16:57:32,377] Trial 3 finished with value: 42.71259898978632 and parameters: {'lr': 0.005690683999391386, 'optimizer': 'Adam', 'fc1_units': 128, 'fc2_units': 32, 'batch_size': 256}. Best is trial 3 with value: 42.71259898978632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Test Accuracy: 42.71%\n",
      "Finished trial #3 with value: 42.71%\n",
      "Best parameters: {'lr': 0.005690683999391386, 'optimizer': 'Adam', 'fc1_units': 128, 'fc2_units': 32, 'batch_size': 256}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-08 16:58:52,614] Trial 4 finished with value: 42.81935658630635 and parameters: {'lr': 7.689204209467373e-05, 'optimizer': 'Adam', 'fc1_units': 256, 'fc2_units': 128, 'batch_size': 64}. Best is trial 4 with value: 42.81935658630635.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Test Accuracy: 42.82%\n",
      "Finished trial #4 with value: 42.82%\n",
      "Best parameters: {'lr': 7.689204209467373e-05, 'optimizer': 'Adam', 'fc1_units': 256, 'fc2_units': 128, 'batch_size': 64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-08 17:00:03,447] Trial 5 finished with value: 43.147596360681334 and parameters: {'lr': 0.000969062169314886, 'optimizer': 'Adam', 'fc1_units': 256, 'fc2_units': 64, 'batch_size': 64}. Best is trial 5 with value: 43.147596360681334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Test Accuracy: 43.15%\n",
      "Finished trial #5 with value: 43.15%\n",
      "Best parameters: {'lr': 0.000969062169314886, 'optimizer': 'Adam', 'fc1_units': 256, 'fc2_units': 64, 'batch_size': 64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-08 17:00:57,794] Trial 6 finished with value: 42.43056772733791 and parameters: {'lr': 4.921966523053483e-05, 'optimizer': 'Adam', 'fc1_units': 64, 'fc2_units': 128, 'batch_size': 64}. Best is trial 5 with value: 43.147596360681334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Test Accuracy: 42.43%\n",
      "Finished trial #5 with value: 43.15%\n",
      "Best parameters: {'lr': 0.000969062169314886, 'optimizer': 'Adam', 'fc1_units': 256, 'fc2_units': 64, 'batch_size': 64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-08 17:01:31,553] Trial 7 finished with value: 42.8129829984544 and parameters: {'lr': 0.008749229872277697, 'optimizer': 'Adam', 'fc1_units': 64, 'fc2_units': 64, 'batch_size': 256}. Best is trial 5 with value: 43.147596360681334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Test Accuracy: 42.81%\n",
      "Finished trial #5 with value: 43.15%\n",
      "Best parameters: {'lr': 0.000969062169314886, 'optimizer': 'Adam', 'fc1_units': 256, 'fc2_units': 64, 'batch_size': 64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-08 17:02:03,447] Trial 8 finished with value: 40.54717251708918 and parameters: {'lr': 3.8566474658189045e-05, 'optimizer': 'Adam', 'fc1_units': 64, 'fc2_units': 32, 'batch_size': 256}. Best is trial 5 with value: 43.147596360681334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Test Accuracy: 40.55%\n",
      "Finished trial #5 with value: 43.15%\n",
      "Best parameters: {'lr': 0.000969062169314886, 'optimizer': 'Adam', 'fc1_units': 256, 'fc2_units': 64, 'batch_size': 64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-08 17:02:52,543] Trial 9 finished with value: 42.891059449640686 and parameters: {'lr': 0.00561722291258225, 'optimizer': 'Adam', 'fc1_units': 256, 'fc2_units': 64, 'batch_size': 256}. Best is trial 5 with value: 43.147596360681334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Test Accuracy: 42.89%\n",
      "Finished trial #5 with value: 43.15%\n",
      "Best parameters: {'lr': 0.000969062169314886, 'optimizer': 'Adam', 'fc1_units': 256, 'fc2_units': 64, 'batch_size': 64}\n",
      "Number of finished trials: 10\n",
      "Best trial: {'lr': 0.000969062169314886, 'optimizer': 'Adam', 'fc1_units': 256, 'fc2_units': 64, 'batch_size': 64}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "# Define the loss function \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters to be optimized\n",
    "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
    "    fc1_units = trial.suggest_categorical('fc1_units', [64, 128, 256])\n",
    "    fc2_units = trial.suggest_categorical('fc2_units', [32, 64, 128])\n",
    "    batch_size = trial.suggest_categorical('batch_size', [64, 128, 256])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Model setup\n",
    "    model = HospitalStayNetOptuna(num_features, num_classes, fc1_units, fc2_units).to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Initialize the variable to store best accuracy for this set of hyper parameters\n",
    "    best_accuracy = 0\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        # We use the latest version of the model here\n",
    "        test_accuracy = calculate_accuracy_optuna(test_loader, model) \n",
    "        if test_accuracy > best_accuracy:\n",
    "            best_accuracy = test_accuracy\n",
    "        \n",
    "        # print epoch number and test accuracy for this epoch\n",
    "        print_threshold = 10\n",
    "        if ((epoch+1) % print_threshold == 0):\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Test Accuracy: {best_accuracy:.2f}%')\n",
    "\n",
    "\n",
    "    return best_accuracy\n",
    "\n",
    "# This method is used by optuna to print intermediate results\n",
    "def print_trial_callback(study, trial):\n",
    "    trial = study.best_trial\n",
    "    print(f\"Finished trial #{trial.number} with value: {trial.value:.2f}%\")\n",
    "    print(f\"Best parameters: {trial.params}\")\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "# Adjust the number of trials here\n",
    "study.optimize(objective, n_trials=10, callbacks=[print_trial_callback])  \n",
    "\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post hyperparameter tuning, our model performance has improved slightly and stands at 43.15%. We also got our optimal parameters in the 5th trial. The best learning rate for our model stands at 0.0096. Further the optimal number of neurons in the first hidden layer is 256 and the second layer is 64. Lastly, the optimal batch size we have got after hyperparameter tuning stands at 64. \n",
    "\n",
    "Feel free to go ahead and build the model with the best parameters.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
